{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">187</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m187\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\n",
      "\n",
      "Summary: The essence of audio-visual segmentation (AVS) lies in locating and\n",
      "delineating sound-emitting objects within a video stream. While\n",
      "Transformer-based methods have shown promise, their handling of long-range\n",
      "dependencies struggles due to quadratic computational costs, presenting a\n",
      "bottleneck in complex scenarios. To overcome this limitation and facilitate\n",
      "complex multi-modal comprehension with linear complexity, we introduce\n",
      "AVS-Mamba, a selective state space model to address the AVS task. Our framework\n",
      "incorporates two key components for video understanding and cross-modal\n",
      "learning: Temporal Mamba Block for sequential video processing and\n",
      "Vision-to-Audio Fusion Block for advanced audio-vision integration. Building on\n",
      "this, we develop the Multi-scale Temporal Encoder, aimed at enhancing the\n",
      "learning of visual features across scales, facilitating the perception of\n",
      "intra- and inter-frame information. To perform multi-modal fusion, we propose\n",
      "the Modality Aggregation Decoder, leveraging the Vision-to-Audio Fusion Block\n",
      "to integrate visual features into audio features across both frame and temporal\n",
      "levels. Further, we adopt the Contextual Integration Pyramid to perform\n",
      "audio-to-vision spatial-temporal context collaboration. Through these\n",
      "innovative contributions, our approach achieves new state-of-the-art results on\n",
      "the AVSBench-object and AVSBench-semantic datasets. Our source code and model\n",
      "weights are available at AVS-Mamba.\n",
      "\n",
      "Page Body: back into its constituent components, aligning them with their\n",
      "original dimensions.\n",
      "b) Temporal Mamba Block: Previous methods [4], [5],\n",
      "[7] mainly focus on cross-scale fusion within individual\n",
      "frames, which does not sufficiently enhance the semantic\n",
      "coherence of spatiotemporal features. To address these limi-\n",
      "tations, we introduce the Temporal Mamba Block to facilitate\n",
      "video-level multi-scale interactions. This module significantly\n",
      "enhances inter-frame connections among feature maps, thereby\n",
      "improving the overall coherence and utility of the extracted\n",
      "features. Fig. 3 presents the architecture of the Temporal\n",
      "Mamba Block, which builds on the VMamba [10] by incorpo-\n",
      "rating depth-wise 3D convolution and the 3D Selective Scan\n",
      "Block to enhance the modeling of the temporal dimension.\n",
      "It processes visual features { Ë†\n",
      "F i\n",
      "v}4\n",
      "i=2 received from the VSS\n",
      "Block, utilizing 3D depth-wise convolution to distill extract\n",
      "temporal-spatial characteristics effectively.\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you know that there's a concept called \"Temporal Mamba\" that's being used in deep learning to improve segmentation and classification tasks in various domains, including medical imaging?\n",
      "\n",
      "In a paper titled \"Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\" [1], the authors introduce a Temporal Mamba Block that enables fine-grained fusion across frames, leading to significant performance improvements. This is especially useful in tasks like audio-visual segmentation, where temporal dependencies play a crucial role.\n",
      "\n",
      "But what's even more fascinating is that the Temporal Mamba concept has been explored in various other domains, such as medical imaging and remote sensing. For instance, in a paper titled \"Vivim: a video vision mamba for medical video object segmentation\" [2], the authors apply the Temporal Mamba idea to medical video object segmentation, achieving impressive results.\n",
      "\n",
      "It's amazing to see how a concept like Temporal Mamba can have such a far-reaching impact across different fields!\n",
      "\n",
      "References:\n",
      "[1] AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\n",
      "[2] Vivim: a video vision mamba for medical video object segmentation\n",
      "\n",
      "How's that for something interesting?"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain 1 Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "\n",
    "# Fill in the TODO to retrieve context from your docstore\n",
    "context_getter = RunnableLambda(\n",
    "    lambda d: docs2str(docstore.similarity_search(d[\"input\"], k=3))\n",
    ")\n",
    "\n",
    "retrieval_chain = {\"input\": (lambda x: x)} | RunnableAssign({\"context\": context_getter})\n",
    "\n",
    "## Chain 2 Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "\n",
    "# Fill in the TODO to generate the final answer from prompt + LLM\n",
    "generator_chain = chat_prompt | llm\n",
    "\n",
    "generator_chain = {\"output\": generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do the Mamba-based approaches in DM-Mamba and AVS-Mamba address the limitations of existing </span>\n",
       "<span style=\"font-weight: bold\">techniques in handling long-range dependencies, and what innovative components and architectures do they introduce </span>\n",
       "<span style=\"font-weight: bold\">to improve performance and efficiency in their respective tasks?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How do the Mamba-based approaches in DM-Mamba and AVS-Mamba address the limitations of existing \u001b[0m\n",
       "\u001b[1mtechniques in handling long-range dependencies, and what innovative components and architectures do they introduce \u001b[0m\n",
       "\u001b[1mto improve performance and efficiency in their respective tasks?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Both DM-Mamba and AVS-Mamba address the limitations of existing techniques in handling long-range </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dependencies by leveraging Mamba, a paradigm for long-range dependency modeling with linear complexity. DM-Mamba </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pioneers the use of Mamba in k-space learning, introducing a circular scanning strategy for spectrum unfolding and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mitigating long-range forgetting. AVS-Mamba, on the other hand, incorporates a Temporal Mamba Block for sequential </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">video processing and a Vision-to-Audio Fusion Block for cross-modal learning. Additionally, both approaches </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">introduce innovative components such as the multi-scale Mamba architecture in DM-Mamba and the Modality Aggregation</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Decoder and Contextual Integration Pyramid in AVS-Mamba, which facilitate the perception of intra- and inter-frame </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">information and enable advanced audio-vision integration, respectively.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Both DM-Mamba and AVS-Mamba address the limitations of existing techniques in handling long-range \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdependencies by leveraging Mamba, a paradigm for long-range dependency modeling with linear complexity. DM-Mamba \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpioneers the use of Mamba in k-space learning, introducing a circular scanning strategy for spectrum unfolding and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmitigating long-range forgetting. AVS-Mamba, on the other hand, incorporates a Temporal Mamba Block for sequential \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvideo processing and a Vision-to-Audio Fusion Block for cross-modal learning. Additionally, both approaches \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mintroduce innovative components such as the multi-scale Mamba architecture in DM-Mamba and the Modality Aggregation\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mDecoder and Contextual Integration Pyramid in AVS-Mamba, which facilitate the perception of intra- and inter-frame \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minformation and enable advanced audio-vision integration, respectively.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: What is the common thread between the Skip Mamba (Skimba) denoiser in monocular 3D semantic scene </span>\n",
       "<span style=\"font-weight: bold\">completion and the Temporal Mamba Block in audio-visual segmentation, and how is it utilized in both applications?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: What is the common thread between the Skip Mamba \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSkimba\u001b[0m\u001b[1m)\u001b[0m\u001b[1m denoiser in monocular 3D semantic scene \u001b[0m\n",
       "\u001b[1mcompletion and the Temporal Mamba Block in audio-visual segmentation, and how is it utilized in both applications?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The Mamba framework, employed in both the Skip Mamba denoiser and the Temporal Mamba Block, offers a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">balanced solution by expanding the receptive field without incurring high memory costs, which is beneficial for 3D </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">semantic scene completion and audio-visual segmentation. Specifically, it is used in the Skip Mamba denoising </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">diffusion network for efficiently processing long-sequence data in 3D scene completion and in the Temporal Mamba </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Block for sequential video processing in audio-visual segmentation.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The Mamba framework, employed in both the Skip Mamba denoiser and the Temporal Mamba Block, offers a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbalanced solution by expanding the receptive field without incurring high memory costs, which is beneficial for 3D \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msemantic scene completion and audio-visual segmentation. Specifically, it is used in the Skip Mamba denoising \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdiffusion network for efficiently processing long-sequence data in 3D scene completion and in the Temporal Mamba \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mBlock for sequential video processing in audio-visual segmentation.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: What is the common thread between the Skip Mamba (Skimba) diffusion model presented in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Skip Mamba </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diffusion for Monocular 3D Semantic Scene Completion\"</span><span style=\"font-weight: bold\"> and the Temporal Mamba Block mentioned in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AVS-Mamba: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"</span><span style=\"font-weight: bold\">?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: What is the common thread between the Skip Mamba \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSkimba\u001b[0m\u001b[1m)\u001b[0m\u001b[1m diffusion model presented in \u001b[0m\u001b[32m\"Skip Mamba \u001b[0m\n",
       "\u001b[32mDiffusion for Monocular 3D Semantic Scene Completion\"\u001b[0m\u001b[1m and the Temporal Mamba Block mentioned in \u001b[0m\u001b[32m\"AVS-Mamba: \u001b[0m\n",
       "\u001b[32mExploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"\u001b[0m\u001b[1m?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Both models employ the concept of </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mamba\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> in their architecture, which suggests the use of a selective </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state space model to facilitate complex processing tasks. In the case of Skip Mamba, it is used for 3D semantic </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">scene completion, while in AVS-Mamba, it is used for audio-visual segmentation. The term </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mamba\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> seems to be </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">associated with advanced state space modeling techniques that enable efficient processing of sequential and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">multi-modal data.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Both models employ the concept of \u001b[0m\u001b[32m\"Mamba\"\u001b[0m\u001b[1;38;2;118;185;0m in their architecture, which suggests the use of a selective \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate space model to facilitate complex processing tasks. In the case of Skip Mamba, it is used for 3D semantic \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mscene completion, while in AVS-Mamba, it is used for audio-visual segmentation. The term \u001b[0m\u001b[32m\"Mamba\"\u001b[0m\u001b[1;38;2;118;185;0m seems to be \u001b[0m\n",
       "\u001b[1;38;2;118;185;0massociated with advanced state space modeling techniques that enable efficient processing of sequential and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmulti-modal data.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How do the Mamba-based approaches in DM-Mamba and AVS-Mamba address the limitations of existing </span>\n",
       "<span style=\"font-weight: bold\">techniques in handling long-range dependencies, and what innovative components and architectures do they introduce </span>\n",
       "<span style=\"font-weight: bold\">to improve performance and efficiency in their respective tasks?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How do the Mamba-based approaches in DM-Mamba and AVS-Mamba address the limitations of existing \u001b[0m\n",
       "\u001b[1mtechniques in handling long-range dependencies, and what innovative components and architectures do they introduce \u001b[0m\n",
       "\u001b[1mto improve performance and efficiency in their respective tasks?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Great question! So, you're wondering how Mamba-based approaches like DM-Mamba and AVS-Mamba tackle the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">challenges of existing techniques in handling long-range dependencies.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">First, let's talk about what Mamba brings to the table. The Selective State Space Model, or Mamba for short, is an </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">innovative framework that's particularly good at capturing long-range contexts with linear computational </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">complexity. This is in contrast to traditional models that often struggle with long-range dependencies.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In AVS-Mamba, the researchers leverage Mamba's ability to process longer sequences in a single pass, which enables </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the exchange of multi-frame video features across various scales. This is a game-changer for audio-visual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">segmentation tasks, as it strengthens the connections between adjacent frames while minimizing interference from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">redundant information in temporally distant frames.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To address the limitations of existing techniques, DM-Mamba and AVS-Mamba introduce some innovative components and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architectures. For instance, AVS-Mamba includes a Multi-scale Temporal Encoder that extracts features across scales</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">within and between frames. This allows the model to capture more nuanced and detailed representations of video </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">features.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Modality Aggregation Decoder is another key component of AVS-Mamba. This decoder features a Vision-to-Audio </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Fusion Block that seamlessly integrates visual data from diverse spatiotemporal contexts into the audio stream. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This is a unique approach that enables the model to take advantage of the strengths of both visual and audio </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">modalities.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Lastly, the Contextual Integration Pyramid is an effective component of AVS-Mamba that combines the visual and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">audio features at different scales to capture cross-scale temporal consistency. This innovative architecture allows</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">AVS-Mamba to achieve state-of-the-art performance in audio-visual segmentation tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In summary, Mamba-based approaches like DM-Mamba and AVS-Mamba address the limitations of existing techniques by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">introducing innovative components and architectures that can handle long-range dependencies with greater efficiency</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and effectiveness.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Quote </span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Quote from AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation]\"</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Great question! So, you're wondering how Mamba-based approaches like DM-Mamba and AVS-Mamba tackle the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mchallenges of existing techniques in handling long-range dependencies.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirst, let's talk about what Mamba brings to the table. The Selective State Space Model, or Mamba for short, is an \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minnovative framework that's particularly good at capturing long-range contexts with linear computational \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomplexity. This is in contrast to traditional models that often struggle with long-range dependencies.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn AVS-Mamba, the researchers leverage Mamba's ability to process longer sequences in a single pass, which enables \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe exchange of multi-frame video features across various scales. This is a game-changer for audio-visual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msegmentation tasks, as it strengthens the connections between adjacent frames while minimizing interference from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mredundant information in temporally distant frames.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo address the limitations of existing techniques, DM-Mamba and AVS-Mamba introduce some innovative components and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitectures. For instance, AVS-Mamba includes a Multi-scale Temporal Encoder that extracts features across scales\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwithin and between frames. This allows the model to capture more nuanced and detailed representations of video \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfeatures.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Modality Aggregation Decoder is another key component of AVS-Mamba. This decoder features a Vision-to-Audio \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFusion Block that seamlessly integrates visual data from diverse spatiotemporal contexts into the audio stream. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThis is a unique approach that enables the model to take advantage of the strengths of both visual and audio \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodalities.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mLastly, the Contextual Integration Pyramid is an effective component of AVS-Mamba that combines the visual and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maudio features at different scales to capture cross-scale temporal consistency. This innovative architecture allows\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAVS-Mamba to achieve state-of-the-art performance in audio-visual segmentation tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn summary, Mamba-based approaches like DM-Mamba and AVS-Mamba address the limitations of existing techniques by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mintroducing innovative components and architectures that can handle long-range dependencies with greater efficiency\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand effectiveness.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Quote \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: What is the common thread between the Skip Mamba (Skimba) denoiser in monocular 3D semantic scene </span>\n",
       "<span style=\"font-weight: bold\">completion and the Temporal Mamba Block in audio-visual segmentation, and how is it utilized in both applications?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: What is the common thread between the Skip Mamba \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSkimba\u001b[0m\u001b[1m)\u001b[0m\u001b[1m denoiser in monocular 3D semantic scene \u001b[0m\n",
       "\u001b[1mcompletion and the Temporal Mamba Block in audio-visual segmentation, and how is it utilized in both applications?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the information I retrieved, it seems that both the Skimba denoiser in 3D semantic scene </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">completion and the Temporal Mamba Block in audio-visual segmentation share a common architecture, which is </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">essentially a modified version of the Mamba block.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As per the quote </span><span style=\"color: #008000; text-decoration-color: #008000\">\"The main difference between Skimba block and Mamba block (Li, Singh, and Grover 2024; Zhu et al. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2024) is that Skimba is optimized for memory-efficient extraction of rich direct and indirect spatial features by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">three STM layers, whereas Mamba captures spatial information from various scan directions, requiring considerable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory.\"</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Temporal Mamba Block, however, doesn't seem to be directly mentioned in this quote. But, it seems that the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Temporal Mamba Block (Ti-Mamba) is a variant of the Mamba Block, and it's designed for temporal information </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">processing. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In both applications, the modified Mamba blocks are used as a key component of the overall technique. They're </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">utilized to capture spatial features or temporal information, and they're an essential part of the architecture. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">However, the specific implementation details might vary depending on the application and requirements.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Regarding the Semantic Block in Skimba, it's a separate type of block devised to handle spatial features before </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">passing them to the Skimba Block. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To directly answer your question, while the Skimba denoiser and Temporal Mamba Block have a common architectural </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">thread through the Mamba Block, it seems they're optimized for different use cases, with Skimba focusing on 3D </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">spatial features and the Temporal Mamba Block focused on temporal information.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the information I retrieved, it seems that both the Skimba denoiser in 3D semantic scene \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcompletion and the Temporal Mamba Block in audio-visual segmentation share a common architecture, which is \u001b[0m\n",
       "\u001b[1;38;2;118;185;0messentially a modified version of the Mamba block.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs per the quote \u001b[0m\u001b[32m\"The main difference between Skimba block and Mamba block \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLi, Singh, and Grover 2024; Zhu et al. \u001b[0m\n",
       "\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is that Skimba is optimized for memory-efficient extraction of rich direct and indirect spatial features by \u001b[0m\n",
       "\u001b[32mthree STM layers, whereas Mamba captures spatial information from various scan directions, requiring considerable \u001b[0m\n",
       "\u001b[32mmemory.\"\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Temporal Mamba Block, however, doesn't seem to be directly mentioned in this quote. But, it seems that the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTemporal Mamba Block \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mTi-Mamba\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m is a variant of the Mamba Block, and it's designed for temporal information \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprocessing. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn both applications, the modified Mamba blocks are used as a key component of the overall technique. They're \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mutilized to capture spatial features or temporal information, and they're an essential part of the architecture. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mHowever, the specific implementation details might vary depending on the application and requirements.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mRegarding the Semantic Block in Skimba, it's a separate type of block devised to handle spatial features before \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpassing them to the Skimba Block. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo directly answer your question, while the Skimba denoiser and Temporal Mamba Block have a common architectural \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthread through the Mamba Block, it seems they're optimized for different use cases, with Skimba focusing on 3D \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspatial features and the Temporal Mamba Block focused on temporal information.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: What is the common thread between the Skip Mamba (Skimba) diffusion model presented in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Skip Mamba </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diffusion for Monocular 3D Semantic Scene Completion\"</span><span style=\"font-weight: bold\"> and the Temporal Mamba Block mentioned in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AVS-Mamba: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"</span><span style=\"font-weight: bold\">?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: What is the common thread between the Skip Mamba \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSkimba\u001b[0m\u001b[1m)\u001b[0m\u001b[1m diffusion model presented in \u001b[0m\u001b[32m\"Skip Mamba \u001b[0m\n",
       "\u001b[32mDiffusion for Monocular 3D Semantic Scene Completion\"\u001b[0m\u001b[1m and the Temporal Mamba Block mentioned in \u001b[0m\u001b[32m\"AVS-Mamba: \u001b[0m\n",
       "\u001b[32mExploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"\u001b[0m\u001b[1m?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Great question! After reviewing the documents, I found a common connection between the two. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The key thread seems to be the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mamba\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> concept itself. In the context of </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Skip Mamba Diffusion for Monocular 3D </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Semantic Scene Completion\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the Skip Mamba (Skimba) diffusion model leverages a Triple Mamba configuration, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inspired by Yang et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Temporal Mamba Block is a component of the AVS-Mamba model. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While the specific applications and architectures differ, it appears that both documents utilize the Mamba concept,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with the Skip Mamba diffusion model in the first document and the Temporal Mamba Block in the second document. The </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mamba concept might be a common thread across different research studies, but more investigation is needed to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">confirm this.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Do you need more clarification or would you like me to expand on this?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Great question! After reviewing the documents, I found a common connection between the two. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe key thread seems to be the \u001b[0m\u001b[32m\"Mamba\"\u001b[0m\u001b[1;38;2;118;185;0m concept itself. In the context of \u001b[0m\u001b[32m\"Skip Mamba Diffusion for Monocular 3D \u001b[0m\n",
       "\u001b[32mSemantic Scene Completion\"\u001b[0m\u001b[1;38;2;118;185;0m, the Skip Mamba \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSkimba\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m diffusion model leverages a Triple Mamba configuration, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minspired by Yang et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2024\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOn the other hand, in \u001b[0m\u001b[32m\"AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"\u001b[0m\u001b[1;38;2;118;185;0m, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTemporal Mamba Block is a component of the AVS-Mamba model. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWhile the specific applications and architectures differ, it appears that both documents utilize the Mamba concept,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith the Skip Mamba diffusion model in the first document and the Temporal Mamba Block in the second document. The \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMamba concept might be a common thread across different research studies, but more investigation is needed to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconfirm this.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mDo you need more clarification or would you like me to expand on this?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    # 1) Parse out the question text from \"Question: ...\"\n",
    "    question_text = q.replace(\"Question:\", \"\").strip()\n",
    "\n",
    "    # 2) Invoke your RAG chain with the parsed question\n",
    "    rag_answer = rag_chain.invoke(question_text)\n",
    "\n",
    "    # 3) Retrieve the final string answer from the chain's output\n",
    "    #rag_answer = rag_answer_dict.get(\"output\", \"\")\n",
    "\n",
    "    # 4) Save the answer for later comparisons\n",
    "    rag_answers.append(rag_answer)\n",
    "\n",
    "    # 5) Print out results\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\") \n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How do the Mamba-based approaches in DM-Mamba and AVS-Mamba address the limitations of existing</span>\n",
       "<span style=\"font-weight: bold\">techniques in handling long-range dependencies, and what innovative components and architectures do they introduce </span>\n",
       "<span style=\"font-weight: bold\">to improve performance and efficiency in their respective tasks?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How do the Mamba-based approaches in DM-Mamba and AVS-Mamba address the limitations of existing\u001b[0m\n",
       "\u001b[1mtechniques in handling long-range dependencies, and what innovative components and architectures do they introduce \u001b[0m\n",
       "\u001b[1mto improve performance and efficiency in their respective tasks?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Both DM-Mamba and AVS-Mamba address the limitations of existing techniques in handling </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">long-range dependencies by leveraging Mamba, a paradigm for long-range dependency modeling with linear complexity. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">DM-Mamba pioneers the use of Mamba in k-space learning, introducing a circular scanning strategy for spectrum </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">unfolding and mitigating long-range forgetting. AVS-Mamba, on the other hand, incorporates a Temporal Mamba Block </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for sequential video processing and a Vision-to-Audio Fusion Block for cross-modal learning. Additionally, both </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approaches introduce innovative components such as the multi-scale Mamba architecture in DM-Mamba and the Modality </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Aggregation Decoder and Contextual Integration Pyramid in AVS-Mamba, which facilitate the perception of intra- and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inter-frame information and enable advanced audio-vision integration, respectively.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Both DM-Mamba and AVS-Mamba address the limitations of existing techniques in handling \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlong-range dependencies by leveraging Mamba, a paradigm for long-range dependency modeling with linear complexity. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mDM-Mamba pioneers the use of Mamba in k-space learning, introducing a circular scanning strategy for spectrum \u001b[0m\n",
       "\u001b[1;38;2;118;185;0munfolding and mitigating long-range forgetting. AVS-Mamba, on the other hand, incorporates a Temporal Mamba Block \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor sequential video processing and a Vision-to-Audio Fusion Block for cross-modal learning. Additionally, both \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproaches introduce innovative components such as the multi-scale Mamba architecture in DM-Mamba and the Modality \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAggregation Decoder and Contextual Integration Pyramid in AVS-Mamba, which facilitate the perception of intra- and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minter-frame information and enable advanced audio-vision integration, respectively.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Great question! So, you're wondering how Mamba-based approaches like DM-Mamba and AVS-Mamba tackle the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">challenges of existing techniques in handling long-range dependencies.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">First, let's talk about what Mamba brings to the table. The Selective State Space Model, or Mamba for short, is an </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">innovative framework that's particularly good at capturing long-range contexts with linear computational </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">complexity. This is in contrast to traditional models that often struggle with long-range dependencies.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In AVS-Mamba, the researchers leverage Mamba's ability to process longer sequences in a single pass, which enables </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the exchange of multi-frame video features across various scales. This is a game-changer for audio-visual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">segmentation tasks, as it strengthens the connections between adjacent frames while minimizing interference from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">redundant information in temporally distant frames.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To address the limitations of existing techniques, DM-Mamba and AVS-Mamba introduce some innovative components and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architectures. For instance, AVS-Mamba includes a Multi-scale Temporal Encoder that extracts features across scales</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">within and between frames. This allows the model to capture more nuanced and detailed representations of video </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">features.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Modality Aggregation Decoder is another key component of AVS-Mamba. This decoder features a Vision-to-Audio </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Fusion Block that seamlessly integrates visual data from diverse spatiotemporal contexts into the audio stream. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This is a unique approach that enables the model to take advantage of the strengths of both visual and audio </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">modalities.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Lastly, the Contextual Integration Pyramid is an effective component of AVS-Mamba that combines the visual and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">audio features at different scales to capture cross-scale temporal consistency. This innovative architecture allows</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">AVS-Mamba to achieve state-of-the-art performance in audio-visual segmentation tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In summary, Mamba-based approaches like DM-Mamba and AVS-Mamba address the limitations of existing techniques by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">introducing innovative components and architectures that can handle long-range dependencies with greater efficiency</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and effectiveness.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Quote </span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Quote from AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation]\"</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Great question! So, you're wondering how Mamba-based approaches like DM-Mamba and AVS-Mamba tackle the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mchallenges of existing techniques in handling long-range dependencies.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirst, let's talk about what Mamba brings to the table. The Selective State Space Model, or Mamba for short, is an \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minnovative framework that's particularly good at capturing long-range contexts with linear computational \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomplexity. This is in contrast to traditional models that often struggle with long-range dependencies.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn AVS-Mamba, the researchers leverage Mamba's ability to process longer sequences in a single pass, which enables \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe exchange of multi-frame video features across various scales. This is a game-changer for audio-visual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msegmentation tasks, as it strengthens the connections between adjacent frames while minimizing interference from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mredundant information in temporally distant frames.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo address the limitations of existing techniques, DM-Mamba and AVS-Mamba introduce some innovative components and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitectures. For instance, AVS-Mamba includes a Multi-scale Temporal Encoder that extracts features across scales\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwithin and between frames. This allows the model to capture more nuanced and detailed representations of video \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfeatures.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Modality Aggregation Decoder is another key component of AVS-Mamba. This decoder features a Vision-to-Audio \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFusion Block that seamlessly integrates visual data from diverse spatiotemporal contexts into the audio stream. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThis is a unique approach that enables the model to take advantage of the strengths of both visual and audio \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodalities.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mLastly, the Contextual Integration Pyramid is an effective component of AVS-Mamba that combines the visual and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maudio features at different scales to capture cross-scale temporal consistency. This innovative architecture allows\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAVS-Mamba to achieve state-of-the-art performance in audio-visual segmentation tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn summary, Mamba-based approaches like DM-Mamba and AVS-Mamba address the limitations of existing techniques by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mintroducing innovative components and architectures that can handle long-range dependencies with greater efficiency\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand effectiveness.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Quote \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">] The second answer is better as it provides a comprehensive explanation of how Mamba-based </span>\n",
       "<span style=\"font-weight: bold\">approaches address limitations of existing techniques, introduces new components, and improves performance and </span>\n",
       "<span style=\"font-weight: bold\">efficiency in their respective tasks, consistently with the ground truth, and offers clarity and coherence in its </span>\n",
       "<span style=\"font-weight: bold\">narrative, although some minor required additional clarification on specific techniques of particular approaches.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m The second answer is better as it provides a comprehensive explanation of how Mamba-based \u001b[0m\n",
       "\u001b[1mapproaches address limitations of existing techniques, introduces new components, and improves performance and \u001b[0m\n",
       "\u001b[1mefficiency in their respective tasks, consistently with the ground truth, and offers clarity and coherence in its \u001b[0m\n",
       "\u001b[1mnarrative, although some minor required additional clarification on specific techniques of particular approaches.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: What is the common thread between the Skip Mamba (Skimba) denoiser in monocular 3D semantic </span>\n",
       "<span style=\"font-weight: bold\">scene completion and the Temporal Mamba Block in audio-visual segmentation, and how is it utilized in both </span>\n",
       "<span style=\"font-weight: bold\">applications?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: What is the common thread between the Skip Mamba \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSkimba\u001b[0m\u001b[1m)\u001b[0m\u001b[1m denoiser in monocular 3D semantic \u001b[0m\n",
       "\u001b[1mscene completion and the Temporal Mamba Block in audio-visual segmentation, and how is it utilized in both \u001b[0m\n",
       "\u001b[1mapplications?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The Mamba framework, employed in both the Skip Mamba denoiser and the Temporal Mamba Block, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">offers a balanced solution by expanding the receptive field without incurring high memory costs, which is </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">beneficial for 3D semantic scene completion and audio-visual segmentation. Specifically, it is used in the Skip </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mamba denoising diffusion network for efficiently processing long-sequence data in 3D scene completion and in the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Temporal Mamba Block for sequential video processing in audio-visual segmentation.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The Mamba framework, employed in both the Skip Mamba denoiser and the Temporal Mamba Block, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moffers a balanced solution by expanding the receptive field without incurring high memory costs, which is \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbeneficial for 3D semantic scene completion and audio-visual segmentation. Specifically, it is used in the Skip \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMamba denoising diffusion network for efficiently processing long-sequence data in 3D scene completion and in the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTemporal Mamba Block for sequential video processing in audio-visual segmentation.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the information I retrieved, it seems that both the Skimba denoiser in 3D semantic scene </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">completion and the Temporal Mamba Block in audio-visual segmentation share a common architecture, which is </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">essentially a modified version of the Mamba block.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As per the quote </span><span style=\"color: #008000; text-decoration-color: #008000\">\"The main difference between Skimba block and Mamba block (Li, Singh, and Grover 2024; Zhu et al. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2024) is that Skimba is optimized for memory-efficient extraction of rich direct and indirect spatial features by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">three STM layers, whereas Mamba captures spatial information from various scan directions, requiring considerable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory.\"</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Temporal Mamba Block, however, doesn't seem to be directly mentioned in this quote. But, it seems that the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Temporal Mamba Block (Ti-Mamba) is a variant of the Mamba Block, and it's designed for temporal information </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">processing. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In both applications, the modified Mamba blocks are used as a key component of the overall technique. They're </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">utilized to capture spatial features or temporal information, and they're an essential part of the architecture. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">However, the specific implementation details might vary depending on the application and requirements.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Regarding the Semantic Block in Skimba, it's a separate type of block devised to handle spatial features before </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">passing them to the Skimba Block. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To directly answer your question, while the Skimba denoiser and Temporal Mamba Block have a common architectural </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">thread through the Mamba Block, it seems they're optimized for different use cases, with Skimba focusing on 3D </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">spatial features and the Temporal Mamba Block focused on temporal information.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the information I retrieved, it seems that both the Skimba denoiser in 3D semantic scene \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcompletion and the Temporal Mamba Block in audio-visual segmentation share a common architecture, which is \u001b[0m\n",
       "\u001b[1;38;2;118;185;0messentially a modified version of the Mamba block.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs per the quote \u001b[0m\u001b[32m\"The main difference between Skimba block and Mamba block \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLi, Singh, and Grover 2024; Zhu et al. \u001b[0m\n",
       "\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is that Skimba is optimized for memory-efficient extraction of rich direct and indirect spatial features by \u001b[0m\n",
       "\u001b[32mthree STM layers, whereas Mamba captures spatial information from various scan directions, requiring considerable \u001b[0m\n",
       "\u001b[32mmemory.\"\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Temporal Mamba Block, however, doesn't seem to be directly mentioned in this quote. But, it seems that the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTemporal Mamba Block \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mTi-Mamba\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m is a variant of the Mamba Block, and it's designed for temporal information \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprocessing. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn both applications, the modified Mamba blocks are used as a key component of the overall technique. They're \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mutilized to capture spatial features or temporal information, and they're an essential part of the architecture. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mHowever, the specific implementation details might vary depending on the application and requirements.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mRegarding the Semantic Block in Skimba, it's a separate type of block devised to handle spatial features before \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpassing them to the Skimba Block. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo directly answer your question, while the Skimba denoiser and Temporal Mamba Block have a common architectural \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthread through the Mamba Block, it seems they're optimized for different use cases, with Skimba focusing on 3D \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspatial features and the Temporal Mamba Block focused on temporal information.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">] The second answer is incorrect because it contains inaccuracies and inconsistencies. </span>\n",
       "<span style=\"font-weight: bold\">Specifically, it references a study by Zhu et al. </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\"> that is not mentioned in the ground truth, and introduces a </span>\n",
       "<span style=\"font-weight: bold\">new component (</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Semantic Block in Skimba\"</span><span style=\"font-weight: bold\">) that is not mentioned in the answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. Additionally, it mentions that </span>\n",
       "<span style=\"font-weight: bold\">the Temporal Mamba Block is a variant of the Mamba Block, which contradicts the statement in the ground truth that </span>\n",
       "<span style=\"font-weight: bold\">both use the same framework but differ in their specific implementation. The second answer also discusses the Mamba</span>\n",
       "<span style=\"font-weight: bold\">Block being used to capture spatial features in Skimba and temporal information in the Temporal Mamba Block, which </span>\n",
       "<span style=\"font-weight: bold\">is inconsistent with the ground truth's statement that both use a balanced solution to expand the receptive field </span>\n",
       "<span style=\"font-weight: bold\">without incurring high memory costs, suggesting that the answer is trying to correct the phrasing to make sense but</span>\n",
       "<span style=\"font-weight: bold\">fails to do so consistently.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m The second answer is incorrect because it contains inaccuracies and inconsistencies. \u001b[0m\n",
       "\u001b[1mSpecifically, it references a study by Zhu et al. \u001b[0m\u001b[1;36m2024\u001b[0m\u001b[1m that is not mentioned in the ground truth, and introduces a \u001b[0m\n",
       "\u001b[1mnew component \u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"Semantic Block in Skimba\"\u001b[0m\u001b[1m)\u001b[0m\u001b[1m that is not mentioned in the answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m. Additionally, it mentions that \u001b[0m\n",
       "\u001b[1mthe Temporal Mamba Block is a variant of the Mamba Block, which contradicts the statement in the ground truth that \u001b[0m\n",
       "\u001b[1mboth use the same framework but differ in their specific implementation. The second answer also discusses the Mamba\u001b[0m\n",
       "\u001b[1mBlock being used to capture spatial features in Skimba and temporal information in the Temporal Mamba Block, which \u001b[0m\n",
       "\u001b[1mis inconsistent with the ground truth's statement that both use a balanced solution to expand the receptive field \u001b[0m\n",
       "\u001b[1mwithout incurring high memory costs, suggesting that the answer is trying to correct the phrasing to make sense but\u001b[0m\n",
       "\u001b[1mfails to do so consistently.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: What is the common thread between the Skip Mamba (Skimba) diffusion model presented in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Skip </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Mamba Diffusion for Monocular 3D Semantic Scene Completion\"</span><span style=\"font-weight: bold\"> and the Temporal Mamba Block mentioned in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AVS-Mamba: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"</span><span style=\"font-weight: bold\">?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: What is the common thread between the Skip Mamba \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSkimba\u001b[0m\u001b[1m)\u001b[0m\u001b[1m diffusion model presented in \u001b[0m\u001b[32m\"Skip \u001b[0m\n",
       "\u001b[32mMamba Diffusion for Monocular 3D Semantic Scene Completion\"\u001b[0m\u001b[1m and the Temporal Mamba Block mentioned in \u001b[0m\u001b[32m\"AVS-Mamba: \u001b[0m\n",
       "\u001b[32mExploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"\u001b[0m\u001b[1m?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Both models employ the concept of </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mamba\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> in their architecture, which suggests the use of a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">selective state space model to facilitate complex processing tasks. In the case of Skip Mamba, it is used for 3D </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">semantic scene completion, while in AVS-Mamba, it is used for audio-visual segmentation. The term </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mamba\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> seems to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">be associated with advanced state space modeling techniques that enable efficient processing of sequential and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">multi-modal data.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Both models employ the concept of \u001b[0m\u001b[32m\"Mamba\"\u001b[0m\u001b[1;38;2;118;185;0m in their architecture, which suggests the use of a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mselective state space model to facilitate complex processing tasks. In the case of Skip Mamba, it is used for 3D \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msemantic scene completion, while in AVS-Mamba, it is used for audio-visual segmentation. The term \u001b[0m\u001b[32m\"Mamba\"\u001b[0m\u001b[1;38;2;118;185;0m seems to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbe associated with advanced state space modeling techniques that enable efficient processing of sequential and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmulti-modal data.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Great question! After reviewing the documents, I found a common connection between the two. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The key thread seems to be the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mamba\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> concept itself. In the context of </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Skip Mamba Diffusion for Monocular 3D </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Semantic Scene Completion\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the Skip Mamba (Skimba) diffusion model leverages a Triple Mamba configuration, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inspired by Yang et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Temporal Mamba Block is a component of the AVS-Mamba model. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While the specific applications and architectures differ, it appears that both documents utilize the Mamba concept,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with the Skip Mamba diffusion model in the first document and the Temporal Mamba Block in the second document. The </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mamba concept might be a common thread across different research studies, but more investigation is needed to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">confirm this.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Do you need more clarification or would you like me to expand on this?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Great question! After reviewing the documents, I found a common connection between the two. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe key thread seems to be the \u001b[0m\u001b[32m\"Mamba\"\u001b[0m\u001b[1;38;2;118;185;0m concept itself. In the context of \u001b[0m\u001b[32m\"Skip Mamba Diffusion for Monocular 3D \u001b[0m\n",
       "\u001b[32mSemantic Scene Completion\"\u001b[0m\u001b[1;38;2;118;185;0m, the Skip Mamba \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSkimba\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m diffusion model leverages a Triple Mamba configuration, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minspired by Yang et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2024\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOn the other hand, in \u001b[0m\u001b[32m\"AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\"\u001b[0m\u001b[1;38;2;118;185;0m, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTemporal Mamba Block is a component of the AVS-Mamba model. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWhile the specific applications and architectures differ, it appears that both documents utilize the Mamba concept,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith the Skip Mamba diffusion model in the first document and the Temporal Mamba Block in the second document. The \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMamba concept might be a common thread across different research studies, but more investigation is needed to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconfirm this.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mDo you need more clarification or would you like me to expand on this?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">] The second answer is better as it directly addresses the question by identifying the common </span>\n",
       "<span style=\"font-weight: bold\">thread, which is the employment of the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mamba\"</span><span style=\"font-weight: bold\"> concept in both models, and provides more specific details regarding</span>\n",
       "<span style=\"font-weight: bold\">the architectures and models, thus improving upon the phrasing and detail of the ground truth, while maintaining </span>\n",
       "<span style=\"font-weight: bold\">consistency with it.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m The second answer is better as it directly addresses the question by identifying the common \u001b[0m\n",
       "\u001b[1mthread, which is the employment of the \u001b[0m\u001b[32m\"Mamba\"\u001b[0m\u001b[1m concept in both models, and provides more specific details regarding\u001b[0m\n",
       "\u001b[1mthe architectures and models, thus improving upon the phrasing and detail of the ground truth, while maintaining \u001b[0m\n",
       "\u001b[1mconsistency with it.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Adapt this prompt for whichever LLM you're actually interested in using. \n",
    "## If it's llama, maybe system message would be good?\n",
    "# Enhanced Evaluation Prompt with Comprehensive System Message and Multiple Examples\n",
    "# Define the Evaluation Prompt with System Message and Clear Instructions\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are an expert evaluator tasked with comparing two answers to the same question. \"\n",
    "     \"Your job is to assess which answer is better based on correctness, completeness, and consistency. \"\n",
    "     \"Be strict in your evaluation, follow the instructions precisely, and ensure the output is concise and clear. \"\n",
    "     \"Do not provide any commentary outside the requested output format.\"),\n",
    "    \n",
    "    (\"user\", \n",
    "     \"\"\"INSTRUCTION:\n",
    "You are given a Question-Answer pair. Your task is to compare the two provided answers:\n",
    "- Answer 1 (Ground Truth): This answer is assumed to be correct and serves as the benchmark.\n",
    "- Answer 2 (RAG Answer): This answer may or may not be true and should be evaluated against Answer 1.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. Accuracy: Does Answer 2 provide correct information consistent with Answer 1?\n",
    "2. Relevance: Does Answer 2 directly answer the question posed?\n",
    "3. Consistency: Does Answer 2 avoid introducing contradictions or irrelevant information?\n",
    "4. Quality: Does Answer 2 improve upon the phrasing, detail, or explanation provided in Answer 1?\n",
    "\n",
    "### Scoring Rules:\n",
    "- **[1]:** Answer 2 is incorrect, incomplete, irrelevant, or introduces inconsistencies.\n",
    "- **[2]:** Answer 2 is correct, consistent, and improves upon Answer 1.\n",
    "\n",
    "### Output Format:\n",
    "Provide the evaluation strictly in the following format:\n",
    "\n",
    "[Score] Justification\n",
    "\n",
    "### Examples:\n",
    "\n",
    "#### Example 1:\n",
    "Question: What is the capital of Italy? Answer 1 (Ground Truth): Rome is the capital of Italy. Answer 2 (RAG Answer): Milan is the capital of Italy.\n",
    "\n",
    "Evaluation: [1] The second answer is incorrect because the capital of Italy is Rome, not Milan.\n",
    "\n",
    "\n",
    "#### Example 2:\n",
    "Question: Explain the significance of photosynthesis in plants. Answer 1 (Ground Truth): Photosynthesis allows plants to convert sunlight into energy, producing glucose and oxygen as by-products. Answer 2 (RAG Answer): Photosynthesis is the process by which plants create energy using sunlight, carbon dioxide, and water, producing glucose and oxygen.\n",
    "\n",
    "Evaluation: [2] The second answer is better as it provides a more complete explanation of photosynthesis while maintaining consistency with the ground truth.\n",
    "\n",
    "#### Example 3:\n",
    "Question: Who wrote the play 'Romeo and Juliet'? Answer 1 (Ground Truth): William Shakespeare wrote 'Romeo and Juliet.' Answer 2 (RAG Answer): The play 'Romeo and Juliet' was written by William Shakespeare.\n",
    "\n",
    "Evaluation: [2] The second answer is better as it uses a clearer structure while maintaining the same factual correctness as the ground truth.\n",
    "\n",
    "\n",
    "Now, evaluate the following Question-Answer pair:\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION:\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/v0.1/docs/modules/agents/) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure you don't have some old session of [`09_langserve.ipynb`](09_langserve.ipynb) already occupying the port. Your assessment requires you to implement the new `/retriever` and `/generator` endpoints!!**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`09_langserve.ipynb`](09_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
